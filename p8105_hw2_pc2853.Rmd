---
title: "p8105_hw2_pc2853"
author: "pengxuan chen"
date: "October 5, 2018"
output: github_document
---

```{r setup , include=FALSE}
library(tidyverse)
options(tibble.print_min = 5)

library(cellranger)
```

------------
#Problem 1 
```{r}
transit_data = 
  read.csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude , station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))

```
This is the description of the data. The data "transit_data" is a manipulated version of the original data with cleaned names and selected variables which are "line, station_name, station_latitude , station_longitude, route1:route11, entry, vending, entrance_type, ada". I mutate the entry variable from character to logical. It is tidy enought in my perspective. I did not use "gather" to combine the routes served, since the columns are just spaces provided to show the routes served based on each station, and the columns cannot be the "key". If the routes columns are gathered, they will lost their correlation to the station in specific. But also there are some values in routes that are NA, as well as some values are blank. 
The dimension of the dataset is that it has `r dim(transit_data)[1]` of rows and `r dim(transit_data)[2]` of columns.

* The number of distinct stations are `r nrow(distinct(transit_data, line, station_name))`. 
* The number of stations that are ada complient is `r sum(transit_data$ada == TRUE)`. 
* The proportion of station entrances / exits without vending allow entrance is `r nrow(subset(transit_data, entry == TRUE & vending == "NO")) / nrow(subset(transit_data, vending == "NO"))`. 


#Problem 2

```{r}
wheel_data = 
  readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", range = cell_cols(1:14)) %>% 
  janitor::clean_names() %>% 
  filter(dumpster != is.na(TRUE)) %>% 
  mutate(sports_ball_int = as.integer(sports_balls))
```

The dataset "wheel_data" is a manipulated version of the original dataset "HealthyHarborWaterWheelTotals2018-7-28.xlsx", which is the newly updated one that already combined 2016&2017 data and has both the year and the month variables. The data is cleaned via excluding the variable that is not correlated specific "dumpster". The number of observation of this dataset is `r nrow(wheel_data)`. The variables are all related to each specific "dumpster" ordered by number, with date occured specified. Also, the precipitation in the dataset is seperated into different trash type, for example, "plastic bottles", "glass bottles", and "cigarette butts" and so on.The variable "sports balls" has been turned into integar by creating a new variable called "sports_balls_int".


* The total precipitation in 2017 in weights is `r sum(subset(wheel_data, year == 2017)$weight_tons)` in tons. 
* The median number of sports balls in a dumpster in 2016 is `r median(subset(wheel_data, year == 2016)$sports_balls)`.

#Prblem 3

```{r p3setup, include=FALSE}

devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data(brfss_smart2010)
```

```{r}
health_data = 
  (brfss_smart2010) %>% 
  janitor::clean_names() %>%  
  select(-class, -topic, -question, -sample_size) %>% 
  select(-confidence_limit_low:-geo_location) %>% 
  filter(response == c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  
  mutate(excellent_or_very_good = excellent + very_good)
```

This is a dataset manipulated from the original online "brfss_smart2010" data. In the cleaning process, the name of the data is converted propably. The variable "class, topic, question, sample size and everyting from lower limit confidence to Geolocation" are excluded from the dataset. Then the elements without a response from "excellent" to "poor" are excluded. In this case I used the command "spread" to set the elements "excellent" to "poor" as columns to revised the data. At the end the new variable combining "excellent" and "very good" as one is added. 


 

